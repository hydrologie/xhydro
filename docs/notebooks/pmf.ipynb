{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xscen\n",
    "\n",
    "from clisops.core.subset import subset_shape\n",
    "\n",
    "from xhydro.pmf.pmf import remove_precip, separate_pr, fix_pmp_year, place_pmf_inputs, maximize_snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import xhydro as xh\n",
    "import xhydro.modelling as xhm\n",
    "\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for determining the notebook folder within a running notebook\n",
    "# This cell is not visible when the documentation is built.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "try:\n",
    "    from _finder import _find_current_folder\n",
    "\n",
    "    notebook_folder = _find_current_folder()\n",
    "except ImportError:\n",
    "    from pathlib import Path\n",
    "\n",
    "    notebook_folder = Path().cwd()\n",
    "\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* On charge la météo\n",
    "* On définit la pmp\n",
    "* On choisit une année de Base\n",
    "* On place la pmp\n",
    "* On enlève la précip solide avant\n",
    "* On enlève la précip solide après\n",
    "* On instancie un model\n",
    "* On fait une sim\n",
    "* On trouve le facteur de neige pour Neige 100\n",
    "* On applique le facteur \n",
    "* On simule\n",
    "*\n",
    "*\n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PMF Analysis with Different Tools\n",
    "\n",
    "In this notebook, we will perform some things that can be done in a Probable Maximum Flood (PMF) analysis. A PMF analysis is not a straigthfoward méthodology, this notebook is thus intended to be an example of different methods. For each methods, we will try to give a brief explanations and highlights its purpose.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Choose a base year**\n",
    "   A reference year is selected to align the meteorological data. This ensures that the PMP event is placed in a consistent context, allowing for accurate comparisons and simulations.\n",
    "\n",
    "   **Purpose**: To provide a baseline for integrating the PMP into the weather data.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Place the PMP**\n",
    "   The PMP event is integrated into the weather data at the appropriate time. This step modifies the precipitation data to include the extreme event, ensuring that the PMP is properly represented in the analysis.\n",
    "\n",
    "   **Purpose**: To simulate the impact of the PMP on the hydrological system.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Remove solid precipitation before**\n",
    "   Any solid precipitation (e.g., snow) that occurred before the PMP event is removed. This step isolates the impact of the PMP by eliminating prior snow accumulation.\n",
    "\n",
    "   **Purpose**: To focus the analysis on the PMP event and its direct effects.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Remove solid precipitation after**\n",
    "   Similarly, solid precipitation that occurred after the PMP event is removed. This ensures that the analysis remains centered on the PMP and avoids interference from subsequent snow events.\n",
    "\n",
    "   **Purpose**: To maintain the integrity of the PMP-focused analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Instantiate a model**\n",
    "   A hydrological model is created and configured. This model will simulate the behavior of the hydrological system under the influence of the PMP and other factors.\n",
    "\n",
    "   **Purpose**: To provide a computational framework for simulating the PMF.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. **Run a simulation**\n",
    "   An initial simulation is performed using the prepared data. This provides a baseline scenario to evaluate the system's response to the PMP.\n",
    "\n",
    "   **Purpose**: To establish a reference point for further analysis and adjustments.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. **Find the snow factor for Snow 100**\n",
    "   The snow factor for a specific scenario (e.g., Snow 100) is calculated. This involves determining the contribution of snow to the overall hydrological response.\n",
    "\n",
    "   **Purpose**: To quantify the role of snow in the PMF analysis and adjust the model accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "### 10. **Apply the snow factor**\n",
    "   The calculated snow factor is applied to the data. This step modifies the input data to account for the effects of snow on the hydrological system.\n",
    "\n",
    "   **Purpose**: To incorporate the snow factor into the analysis and improve the accuracy of the simulation.\n",
    "\n",
    "---\n",
    "\n",
    "### 11. **Run the simulation**\n",
    "   A new simulation is performed with the adjusted data. This step evaluates the hydrological system's response to the PMP, including the effects of the snow factor.\n",
    "\n",
    "   **Purpose**: To generate the final results of the PMF analysis and assess the system's behavior under extreme conditions.\n",
    "\n",
    "---\n",
    "\n",
    "By following these steps, we systematically analyze the Probable Maximum Flood scenario, leveraging various tools and methods to ensure a comprehensive and accurate assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **Define the PMP**\n",
    "   Let define some inputs for the PMF, here we'll define :\n",
    "   * Probable Maximum Precipitation (pmp); \n",
    "   * 100 year return period precipitation (p100); \n",
    "   * critical temperature sequences (min_temp, mean_temp, max_temp);\n",
    "   * probable maximum snow accumulation (pmsa);\n",
    "   * 100 year return period snow (swe100). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define input dictionaries\n",
    "pmsa = {\n",
    "    'Catchment-1': 443,\n",
    "}\n",
    "pmp = {\n",
    "    'C1': {\n",
    "        'Catchment-1': {'J1': 32.0, 'J2': 191.1, 'J3': 38.8},\n",
    "    },\n",
    "\n",
    "}\n",
    "t_crit_17_31_mai = {\n",
    "    'max_temp': {\n",
    "        'Catchment-1': [16.7, 17.3, 17.6, 17.8, 18.0, 18.1, 18.4, 18.8, 26.3, 29.5, 33.3, 20.5, 4.5, 5.0, 4.0],\n",
    "    },\n",
    "    'mean_temp': {\n",
    "        'Catchment-1': [10.8, 10.8, 10.8, 10.8, 10.8, 10.8, 10.9, 11.2, 15.3, 16.9, 18.9, 12.2, 4.5, 5.0, 4.0],\n",
    "    },\n",
    "    'min_temp': {\n",
    "        'Catchment-1': [4.9, 4.3, 4.0, 3.8, 3.6, 3.5, 3.4, 3.6, 4.3, 4.3, 4.5, 3.9, 4.5, 5.0, 4.0],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare arrays and coordinates\n",
    "bvs = list(pmsa.keys())\n",
    "time = pd.date_range(start='2010-05-17', periods=15, freq='D')\n",
    "center = list(pmp.keys())\n",
    "\n",
    "data = {\n",
    "\n",
    "    'pmsa': (['bv'], [pmsa[st] for st in bvs]),\n",
    "    'max_temp': (['bv', 'time'], np.zeros((len(bvs), 15))),\n",
    "    'mean_temp': (['bv', 'time'], np.zeros((len(bvs), 15))),\n",
    "    'min_temp': (['bv', 'time'], np.zeros((len(bvs), 15))),\n",
    "    'pmp': (['bv', 'time', 'center'], np.zeros((len(bvs), 15, len(center)))),\n",
    "}\n",
    "\n",
    "for i, st in enumerate(bvs):\n",
    "    data['max_temp'][1][i, :] = t_crit_17_31_mai['max_temp'][st]\n",
    "    data['mean_temp'][1][i, :] = t_crit_17_31_mai['mean_temp'][st]\n",
    "    data['min_temp'][1][i, :] = t_crit_17_31_mai['min_temp'][st]\n",
    "    for j, center_key in enumerate(center):\n",
    "        data['pmp'][1][i, -3:, j] = [\n",
    "            pmp[center_key][st]['J1'],\n",
    "            pmp[center_key][st]['J2'],\n",
    "            pmp[center_key][st]['J3']\n",
    "        ]\n",
    "\n",
    "ds_pmf_inputs = xr.Dataset(data, coords={'bv': bvs, 'time': time, 'center': center})\n",
    "ds_pmf_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **Load the weather data**\n",
    "   In this step, we import the meteorological data required for the analysis. This includes precipitation, temperature, and other relevant variables. The data is typically loaded from a file or a database and is structured to cover the time period of interest. We here choose daily ERA5_land\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/datasets/reanalyses/day_ERA5-Land_NAM.ncml\"\n",
    "era = xr.open_dataset(path_to_file)\n",
    "era = era[[\"tas\", \"tasmin\",\"tasmax\",\"pr\"]]\n",
    "era"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets clip our meteorology on the Kipawa Watershed and lets only keep 10 years of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file('Kipawa_poly.json')\n",
    "gdf = gdf.to_crs(\"epsg:4326\")\n",
    "date_debut = '2010-01-01'\n",
    "date_fin = '2020-01-01'\n",
    "ds_meteo = subset_shape(era, shape=gdf, start_date=date_debut, end_date=date_fin).mean(dim=['lat', 'lon'])\n",
    "ds_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the clipping, our dataset lost its attribute, lets put them back as we'll need them for unit conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ds_meteo:\n",
    "    ds_meteo[var].attrs = era[var].attrs\n",
    "ds_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put everything in °C and mm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_meteo = xscen.utils.change_units(ds_meteo, {\"tas\": \"degC\", \"tasmin\": \"degC\", \"tasmax\": \"degC\",\"pr\": \"mm\"})\n",
    "ds_meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `ds_pmf_inputs` was arbitrarily placed at 2010, if we want to apply the pmp to another year, the function `fix_pmp_year` does just that. For example here 2012. It can be applied on the dataset or on a data array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pmp = fix_pmp_year(ds_pmf_inputs, 2012)\n",
    "ds_pmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_pmp = fix_pmp_year(ds_pmf_inputs['pmp'], 2012)\n",
    "da_pmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To place inputs in the loaded meteorology, both datasets need to have matching names, here we place the pmp, but it could be a p100 or whatever we want to place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmp = ds_pmf_inputs[['max_temp', 'mean_temp', 'min_temp', 'pmp']].rename({'max_temp':'tasmax', 'min_temp':'tasmin', 'mean_temp':'tas', 'pmp':'pr'})\n",
    "pmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that both datasets have matching variables and units, we can place the inputs in our meteo dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = place_pmf_inputs(pmp, ds_meteo)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the pmp was indeed place in may 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.pr.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the temperature, it's not so obvious, but if we recall, we had this sequence : <br>\n",
    "**10.8, 10.8, 10.8, 10.8, 10.8, 10.8, 10.9, 11.2, 15.3, 16.9, 18.9, 12.2, 4.5, 5.0, 4.0** for mean temperature from may 17 to mai 31. <br>\n",
    "Plotting a zoomed period, and showing the sequence between the red lines, we see that it's been replaced correctly as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Plot the data\n",
    "ax = ds.tas.sel(time=slice('2012-05-10', '2012-06-05')).plot()\n",
    "may_17 = pd.Timestamp('2012-05-17')\n",
    "may_31 = pd.Timestamp('2012-05-31')\n",
    "# Add red dotted vertical lines at May 17 and May 31\n",
    "plt.axvline(x=may_17, color='red', linestyle='--')\n",
    "plt.axvline(x=may_31, color='red', linestyle='--')\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing that is often needed is to remove precipitation, it can be used to add a dry period before or after our event <br>\n",
    "The `remove_precip` function does that, it takes the desired dataarray and a `pandas.date_range`. It would also work on temperature (or any other variable), but I'm not sure when that could be relevant. By default it will only remove for the dates specified, but it can remove specific date everyyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = pd.date_range(start='2012-06-01', periods=150, freq='D')\n",
    "remove_precip(ds['pr'], time, each_year=False).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_precip(ds['pr'], time, each_year=True).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hru = xh.gis.watershed_to_raven_hru(watershed=gdf, unique_id='ID_POULPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the models, you may need to feed it total precipitation or snow and rainfall. While total precipitation can be obtanined from rainfall and snowfall with a straigtfooward sum, spliiting precipitation into snow and rain is a little more complex. We implemented two formulas to do the splitting <br> The input variable is kept but two new variable appended _rain and _snow are added. The algorithm used and the temperature transition specified is also kept in the metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rs = separate_pr(ds,'pr','tasmin','tasmax').drop('pr')\n",
    "ds_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rs = ds_rs.assign_coords({\"altitude\": 450, \"lat\": 46, \"lon\": -72})\n",
    "ds_rs[\"altitude\"].attrs[\"units\"] = \"m\"\n",
    "ds_rs[\"lat\"].attrs[\"units\"] = \"degrees_north\"\n",
    "ds_rs[\"lon\"].attrs[\"units\"] = \"degrees_east\"\n",
    "ds_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_rs.squeeze().dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use the 'save_as' argument to save the new file(s) in your project folder.\n",
    "ds_reformatted, config = xh.modelling.format_input(\n",
    "    ds_rs.rename({'pr_snow':'snowfall', 'pr_rain':'rainfall'}),\n",
    "    \"GR4JCN\",\n",
    "    save_as=notebook_folder / \"_data\" / \"meteo_hmr.nc\",\n",
    ")\n",
    "ds_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"model_name\": \"GR4JCN\",\n",
    "    \"parameters\": [\n",
    "        0.529,\n",
    "        -3.396,\n",
    "        407.29,\n",
    "        1.072,\n",
    "        16.9,\n",
    "        0.947,\n",
    "    ],  # GR4JCN has 6 parameters, others might have more\n",
    "    \"global_parameter\": {\"AVG_ANNUAL_SNOW\": 100.00},\n",
    "    \"hru\": hru,\n",
    "    \"start_date\": \"2010-01-01\",\n",
    "    \"end_date\": \"2013-12-31\",\n",
    "    \"RainSnowFraction\": \"RAINSNOW_DINGMAN\",\n",
    "    \"Evaporation\": \"PET_HARGREAVES_1985\",\n",
    "    **config,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = xhm.hydrological_model(model_config)\n",
    "ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out = ht.run()\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximize_snow(ds_reformatted, model_config,2012, 422)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
