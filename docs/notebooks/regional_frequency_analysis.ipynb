{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency analysis module - Regional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xdatasets as xd\n",
    "from lmoments3.distr import KappaGen\n",
    "from sklearn.cluster import HDBSCAN, OPTICS, AgglomerativeClustering\n",
    "\n",
    "import xhydro as xh\n",
    "import xhydro.frequency_analysis as xhfa\n",
    "import xhydro.gis as xhgis\n",
    "from xhydro.frequency_analysis.regional import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page demonstrate how to use the `xhydro` package to perform regional frequency analysis on a dataset of streamflow data. The first steps will be similar to the local frequency analysis example, but we will keep it simple to focus on the regional frequency analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with getting the 02 region stations that are natural and have a minimum duration of 15 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = (\n",
    "    xd.Query(\n",
    "        **{\n",
    "            \"datasets\": {\n",
    "                \"deh\": {\n",
    "                    \"id\": [\"02*\"],\n",
    "                    \"regulated\": [\"Natural\"],\n",
    "                    \"variables\": [\"streamflow\"],\n",
    "                }\n",
    "            },\n",
    "            \"time\": {\"start\": \"1970-01-01\", \"minimum_duration\": (15 * 365, \"d\")},\n",
    "        }\n",
    "    )\n",
    "    .data.squeeze()\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# This dataset lacks some attributes, so let's add them.\n",
    "ds[\"id\"].attrs[\"cf_role\"] = \"timeseries_id\"\n",
    "ds[\"streamflow\"].attrs = {\n",
    "    \"long_name\": \"Streamflow\",\n",
    "    \"units\": \"m3 s-1\",\n",
    "    \"standard_name\": \"water_volume_transport_in_river_channel\",\n",
    "    \"cell_methods\": \"time: mean\",\n",
    "}\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we hide years with more than 15% of missing data and get yearly max and spring max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeargs = {\n",
    "    \"spring\": {\"date_bounds\": [\"02-11\", \"06-19\"]},\n",
    "    \"annual\": {},\n",
    "}\n",
    "\n",
    "ds_4fa = xh.indicators.get_yearly_op(\n",
    "    ds, op=\"max\", timeargs=timeargs, missing=\"pct\", missing_options={\"tolerance\": 0.15}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainatory variables\n",
    "\n",
    "### a) Extraction using `xhydro.gis`\n",
    "\n",
    "Regional frequency analyses rely on explanatory variables to link the information at the various sites. For this example, we'll use catchment properties, but other variables sur as climatological averages or land use data could also be used. Refer to the GIS example for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = xd.Query(\n",
    "    **{\n",
    "        \"datasets\": {\n",
    "            \"deh_polygons\": {\n",
    "                \"id\": [\"02*\"],\n",
    "                \"regulated\": [\"Natural\"],\n",
    "                \"variables\": [\"streamflow\"],\n",
    "            }\n",
    "        },\n",
    "        \"time\": {\"start\": \"1970-01-01\", \"minimum_duration\": (15 * 365, \"d\")},\n",
    "    }\n",
    ").data.reset_index()\n",
    "\n",
    "dswp = xhgis.watershed_properties(\n",
    "    gdf[[\"Station\", \"geometry\"]], unique_id=\"Station\", output_format=\"xarray\"\n",
    ")\n",
    "cent = dswp[\"centroid\"].to_numpy()\n",
    "lon = [ele[0] for ele in cent]\n",
    "lat = [ele[1] for ele in cent]\n",
    "dswp = dswp.assign(lon=(\"Station\", lon))\n",
    "dswp = dswp.assign(lat=(\"Station\", lat))\n",
    "dswp = dswp.drop(\"centroid\")\n",
    "dswp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Principal component analysis\n",
    "To do our regional frequency analysis, we'll process the data with a principal component analysis (PCA) to reduce the dimensionality of our dataset:\n",
    "The function `xhydro.regional.fit_pca` takes a `xarray.Dataset` as input and returns a `xarray.Dataset` with the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pca = xhfa.regional.fit_pca(dswp, n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the correlation is close to 0 between the components, which means that the first 3 components are independent enough to be used for the rest of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_dataframe(name=\"value\").reset_index().pivot(\n",
    "    index=\"Station\", columns=\"components\"\n",
    ").corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Clustering\n",
    "In this example we'll use `AgglomerativeClustering`, but other methods would also provide valid results. The regional clustering itself is performed using xhfa.regional.get_group_from_fit, which can take the arguments of the skleanr functions as a dictionnary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = xhfa.regional.get_group_from_fit(\n",
    "    AgglomerativeClustering, {\"n_clusters\": 3}, data\n",
    ")\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional analysis\n",
    "**Hosking and Wallis** developed a method for regional frequency analysis that uses L-moments to analyze extreme values across different regions. Here’s a concise overview:\n",
    "1. **L-Moments**: L-moments are summary statistics derived from linear combinations of order statistics. They are less sensitive to outliers compared to traditional moments (like mean and variance) and provide more robust estimates, especially for small sample sizes.\n",
    "2. **Regional Frequency Analysis**: This approach involves pooling data from multiple sites or regions to determine the frequency distribution of extreme events, such as floods. Hosking and Wallis’s methods involve estimating the parameters of regional frequency distributions and evaluating the fit of these distributions to the data.\n",
    "3. **Regional L-Moments**: These are used to summarize data from various sites within a region. By applying L-moment-based methods, parameters can be estimated, and the frequency of extreme events can be assessed across the region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the L-moments for each station\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_moment = calc_moments(ds_4fa)\n",
    "ds_moment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to reshape our datasets of annual maximums and L-moments according to the groupings found using the clustering algorithm. Since there is no convention on the name of that new dimension, it has been decided in xHydro that it would need to be called `group_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_groups = group_ds(ds_4fa, groups)\n",
    "ds_moments_groups = group_ds(ds_moment, groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H-Score (Homogeneity Score)\n",
    "\n",
    "The **H-Score** measures the homogeneity of data across different sites or regions relative to the regional model:\n",
    "\n",
    "- **H < 1: Homogeneous** - Indicates that data from different sites are quite similar and fit well with the regional model. This suggests that the model is appropriate for the region as a whole.\n",
    "\n",
    "- **1 ≤ H < 2: Maybe Homogeneous** - Suggests some degree of heterogeneity, but the data might still fit reasonably well with the regional model. There could be some variations that the model does not fully capture.\n",
    "\n",
    "- **H ≥ 2: Heterogeneous** - Indicates significant differences between sites or regions, suggesting that the model may not be suitable for all the data. The regions might be too diverse, or the model might need adjustments.\n",
    "\n",
    "### Z-Score (Goodness of Fit)\n",
    "\n",
    "The **Z-Score** assesses how well the theoretical distribution (based on the regional model) fits the observed data:\n",
    "\n",
    "- **Z-Score Calculation**: This score quantifies the discrepancy between observed and expected values, standardized by their variability. It indicates whether the differences are statistically significant.\n",
    "\n",
    "- **Interpretation**:\n",
    "\n",
    "    - **Low Z-Score**: A good fit of the model to the observed data. Typically, an absolute value of the Z-Score below 1.64 suggests that the model is appropriate and the fit is statistically acceptable.\n",
    "    \n",
    "    - **High Z-Score**: Indicates significant discrepancies between the observed and expected values. An absolute value above 1.64 suggests that the model may not fit the data well, and adjustments might be necessary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate H and Z, we also need a `KappaGen` object from the lmoment3 librairy. This librairy is not part of the xhydro package, so it need to be installed seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kap = KappaGen()\n",
    "ds_H_Z = calc_h_z(ds_groups, ds_moments_groups, kap)\n",
    "ds_H_Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data to only include the data that has H and Z below the thresholds. The thresholds can be specified but are by default respectively to 1 and 1.64 for H and Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_h_z(ds_H_Z)\n",
    "ds_groups_H1 = ds_groups.where(mask).load()\n",
    "ds_moments_groups_H1 = ds_moments_groups.where(mask).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centiles and return periods :\n",
    "centiles = [x / 100.0 for x in range(101)]\n",
    "return_periods = [\n",
    "    2,\n",
    "    10,\n",
    "    100,\n",
    "    1000,\n",
    "    10000,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the return periods for each group and return period. Also since we dont want to do our analyssis on really small regions, `remove_small_regions` removes any region below a certain threshold. By default this threshold is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_T = calculate_rp_from_afr(ds_groups_H1, ds_moments_groups_H1, return_periods)\n",
    "Q_T = remove_small_regions(Q_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot, let see what it looks like on 023401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg = Q_T.sel(id=\"023401\").dropna(dim=\"group_id\", how=\"all\")\n",
    "reg = Q_reg.streamflow_max_annual.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare local and regional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_loc = xhfa.local.fit(ds_4fa)\n",
    "Q_loc = xhfa.local.parametric_quantiles(params_loc, return_periods)\n",
    "loc = Q_loc.sel(id=\"023401\", scipy_dist=\"genextreme\").streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 4))\n",
    "plt.plot(reg.return_period.values, reg.values, \"blue\")\n",
    "plt.plot(loc.return_period.values, loc.values, \"red\")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.xlabel(\"Return period (years)\")\n",
    "plt.ylabel(\"Discharge (m$^3$/s)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainties\n",
    "## Local frequency analysis uncertainties\n",
    "To add some uncertainities, we will work with only one catchment and two distributions as uncertainities can be intensive in computation.\n",
    "We select the station 023401, and distribution 'genextreme' and 'pearson3'. \n",
    "\n",
    "For the local frequency analysis, we need to fit the distribution so the calulting time can be long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4fa_one_station = ds_4fa.sel(id=\"023401\")\n",
    "params_loc_one_station = params_loc.sel(\n",
    "    id=\"023401\", scipy_dist=[\"genextreme\", \"pearson3\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstraping the observations\n",
    "A way to get uncertainities is to bootstrap the observations 200 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_4fa_iter = xhfa.uncertainities.boostrap_obs(ds_4fa_one_station, 200)\n",
    "params_boot_obs = xhfa.local.fit(ds_4fa_iter, distributions=[\"genextreme\", \"pearson3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_boot_obs = xhfa.local.parametric_quantiles(\n",
    "    params_boot_obs.load(), return_periods\n",
    ").squeeze()\n",
    "Q_boot_obs = Q_boot_obs.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the fitted distributions\n",
    "Here, instead of resampling the observations, we resample the fitted distributions 200 times to get the uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = xhfa.uncertainities.boostrap_dist(\n",
    "    ds_4fa_one_station, params_loc_one_station, 200\n",
    ")\n",
    "params_boot_dist = xhfa.uncertainities.fit_boot_dist(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_boot_dist = xhfa.local.parametric_quantiles(\n",
    "    params_boot_dist.load(), return_periods\n",
    ").squeeze()\n",
    "Q_boot_dist = Q_boot_dist.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dist = Q_boot_dist.sel(scipy_dist=\"genextreme\")\n",
    "loc_obs = Q_boot_obs.sel(scipy_dist=\"genextreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figheight(4)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax.plot(reg.return_period.values, reg.values, \"blue\", label=\"Regional\")\n",
    "ax.plot(\n",
    "    loc_obs.return_period.values,\n",
    "    loc_obs.quantile(0.5, \"samples\"),\n",
    "    \"red\",\n",
    "    label=\"bootstrap obs\",\n",
    ")\n",
    "loc_obs_05 = loc_obs.quantile(0.05, \"samples\")\n",
    "loc_obs_95 = loc_obs.quantile(0.95, \"samples\")\n",
    "ax.fill_between(loc_dist.return_period.values, loc_obs_05, loc_obs_95, alpha=0.2)\n",
    "loc_dist_05 = loc_dist.quantile(0.05, \"samples\")\n",
    "loc_dist_95 = loc_dist.quantile(0.95, \"samples\")\n",
    "ax.fill_between(loc_dist.return_period.values, loc_dist_05, loc_dist_95, alpha=0.2)\n",
    "ax.plot(\n",
    "    loc_dist.return_period.values,\n",
    "    loc_dist.quantile(0.5, \"samples\"),\n",
    "    \"green\",\n",
    "    label=\"bootstrap dist\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.grid(visible=True)\n",
    "plt.xlabel(\"Return period (years)\")\n",
    "plt.ylabel(\"Discharge (m$^3$/s)\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regional frequency analysis uncertainties\n",
    "### Bootstraping the observations\n",
    "\n",
    "For the regional analysis, we again use `boostrap_obs` to resample the observations, but, this time, it's much faster as no fit is involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_reg_samples = xhfa.uncertainities.boostrap_obs(ds_4fa, 200)\n",
    "ds_moments_iter = xhfa.uncertainities.calc_moments_iter(ds_reg_samples).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg_boot = xhfa.uncertainities.calc_q_iter(\n",
    "    \"023401\", \"streamflow_max_annual\", ds_groups_H1, ds_moments_iter, return_periods\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_boot = Q_reg_boot.streamflow_max_annual.sel(id=\"023401\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we'll do a few plots to illustrate the results, let's make a function to somplify things a litle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ds_with_CI(\n",
    "    ds_list, CI_dim_list, color_list, label_list, x_label, y_label, title=None\n",
    "):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figheight(4)\n",
    "    fig.set_figwidth(15)\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.grid(visible=True)\n",
    "    for i, ds in enumerate(ds_list):\n",
    "        x = ds.return_period.values\n",
    "        CI_dim = CI_dim_list[i]\n",
    "        y_5 = ds.quantile(0.5, CI_dim)\n",
    "        y_05 = ds.quantile(0.05, CI_dim)\n",
    "        y_95 = ds.quantile(0.95, CI_dim)\n",
    "        color = color_list[i]\n",
    "        label = label_list[i]\n",
    "        plt.plot(x, y_5, color, label=label)\n",
    "        ax.fill_between(x, y_05, y_95, alpha=0.2, color=color)\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ds_with_CI(\n",
    "    [loc_obs, loc_dist, reg_boot],\n",
    "    [\"samples\", \"samples\", \"samples\"],\n",
    "    [\"blue\", \"green\", \"red\"],\n",
    "    [\"bootstrap obs\", \"bootstrap dist\", \"Regional bootstrap\"],\n",
    "    \"Return period (years)\",\n",
    "    \"Discharge (m$^3$/s)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple regions\n",
    "Another way to get the uncertainty is to have many regions for one catchement of interest. We can achive this by trying different clustering methods. Or by performing a jackknife on the station list. We dont do too many tests here since it can take quite a while to run and the goal is just to illustrate the possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try three clustering methods and for each method, we'll try to change some of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM = {\n",
    "    AgglomerativeClustering: {\"arg_name\": \"n_clusters\", \"range\": range(2, 12)},\n",
    "    HDBSCAN: {\"arg_name\": \"min_cluster_size\", \"range\": range(6, 7)},\n",
    "    OPTICS: {\"arg_name\": \"min_samples\", \"range\": range(4, 5)},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generaste stations combination by removing 0-n stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "combinations_list = xhfa.uncertainities.generate_combinations(data, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our station instead of beein in one region, will be in many of the regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = []\n",
    "\n",
    "for model in [AgglomerativeClustering, HDBSCAN, OPTICS]:\n",
    "\n",
    "    for p in PARAM[model][\"range\"]:\n",
    "        d_param = {}\n",
    "        d_param[PARAM[model][\"arg_name\"]] = p\n",
    "        for combination in combinations_list:\n",
    "            # Extract data for the current combination\n",
    "            data_com = data.sel(Station=list(combination))\n",
    "            # Get groups from the fit and add to the list\n",
    "            groups = groups + get_group_from_fit(model, d_param, data_com)\n",
    "unique_groups = [list(x) for x in {tuple(x) for x in groups}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followin steps are similar to the previous one, just with more regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_groups = group_ds(ds_4fa, unique_groups)\n",
    "ds_moments_groups = group_ds(ds_moment, unique_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kap = KappaGen()\n",
    "ds_H_Z = calc_h_z(ds_groups, ds_moments_groups, kap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask_h_z(ds_H_Z)\n",
    "ds_groups_H1 = ds_groups.where(mask).load()\n",
    "ds_moments_groups_H1 = ds_moments_groups.where(mask).load()\n",
    "\n",
    "Q_T = calculate_rp_from_afr(ds_groups_H1, ds_moments_groups_H1, return_periods)\n",
    "Q_T = remove_small_regions(Q_T)\n",
    "\n",
    "Q = Q_T.sel(id=\"023401\").dropna(dim=\"group_id\", how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_multiple_region = Q.streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_moment = calc_moments(ds_4fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ds_with_CI(\n",
    "    [loc_obs, loc_dist, regional_multiple_region],\n",
    "    [\"samples\", \"samples\", \"group_id\"],\n",
    "    [\"blue\", \"green\", \"red\"],\n",
    "    [\"bootstrap obs\", \"bootstrap dist\", \"regional_multiple_region\"],\n",
    "    \"Return period (years)\",\n",
    "    \"Discharge (m$^3$/s)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining bootstrap and multiple regions\n",
    "\n",
    "calc_q_iter will check in how many `group_id` the station is present, and stack it with samples.\n",
    "In this case, it will be stacked with 200 bootstraps, and we have 535 regions, so 104000 samples are generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_reg_boot = xhfa.uncertainities.calc_q_iter(\n",
    "    \"023401\", \"streamflow_max_annual\", ds_groups_H1, ds_moments_iter, return_periods\n",
    ")\n",
    "Q_reg_boot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_multiple_region_boot = Q_reg_boot.sel(id=\"023401\").streamflow_max_annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ds_with_CI(\n",
    "    [loc_obs, loc_dist, regional_multiple_region, regional_multiple_region_boot],\n",
    "    [\"samples\", \"samples\", \"group_id\", \"samples\"],\n",
    "    [\"blue\", \"green\", \"red\", \"black\"],\n",
    "    [\n",
    "        \"bootstrap obs\",\n",
    "        \"bootstrap dist\",\n",
    "        \"regional_multiple_region\",\n",
    "        \"regional_multiple_region_boot\",\n",
    "    ],\n",
    "    \"Return period (years)\",\n",
    "    \"Discharge (m$^3$/s)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
