{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Use Case Example\n",
    "\n",
    "This example illustrates a use case that covers the essential steps involved in building a hydrological model and conducting a climate change analysis:\n",
    "\n",
    "- **Identification of the watershed and its key characteristics**\n",
    "  - Beaurivage watershed in Southern Quebec, at the location of the 023401 streamflow gauge.\n",
    "- **Collection of observed data**\n",
    "  - ERA5-Land and streamflow gauge data.\n",
    "- **Preparation and calibration of the hydrological model**\n",
    "  - GR4JCN emulated by the Raven hydrological framework.\n",
    "- **Calculation of hydrological indicators**\n",
    "  - Mean summer flow\n",
    "  - Mean monthly flow\n",
    "  - 20- and 100-year maximum flow\n",
    "  - 2-year minimum 7-day average summer flow\n",
    "- **Assessment of the impact of climate change**\n",
    "  - Bias-adjusted CMIP6 simulations from the ESPO-G6-R2 dataset\n",
    "\n",
    "## Identification of a watershed and its characteristics\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>INFO</b>\n",
    "\n",
    "For more information on this section and available options, consult the [GIS module page](https://xhydro.readthedocs.io/en/latest/notebooks/gis.html#GIS-module).\n",
    "\n",
    "</div>\n",
    "\n",
    "This first step is highly dependent on the hydrological model. Since we will use GR4JCN in our example, we need to obtain the drainage area, centroid coordinates, and elevation. We'll also need the watershed delineation to extract the meteorological data.\n",
    "\n",
    "- Delineation of the watershed and basic characteristics: `xhydro.gis.watershed_delineation` and `xhydro.gis.watershed_properties`\n",
    "- Elevation: `xhydro.gis.surface_properties`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hidden"
    ]
   },
   "outputs": [],
   "source": [
    "# Workaround for determining the notebook folder within a running notebook\n",
    "# This cell is not visible when the documentation is built.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "try:\n",
    "    from _finder import _find_current_folder\n",
    "\n",
    "    notebook_folder = _find_current_folder()\n",
    "except ImportError:\n",
    "    from pathlib import Path\n",
    "\n",
    "    notebook_folder = Path().cwd()\n",
    "\n",
    "(notebook_folder / \"_data\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import leafmap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import xhydro.gis as xhgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watershed delineation\n",
    "coords = (-71.28878, 46.65692)\n",
    "m = leafmap.Map(center=(coords[1], coords[0]), zoom=8)\n",
    "gdf = xhgis.watershed_delineation(coordinates=coords, map=m)\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watershed properties\n",
    "gdf_wat = xhgis.watershed_properties(gdf)\n",
    "gdf_wat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surface properties\n",
    "gdf_surf = xhgis.surface_properties(gdf)\n",
    "gdf_surf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information for GR4JCN\n",
    "drainage_area = np.array([(gdf_wat[\"area\"] / 1000000).iloc[0]])\n",
    "latitude = np.array([gdf_wat.centroid[0][1]])\n",
    "longitude = np.array([gdf_wat.centroid[0][0]])\n",
    "elevation = np.array([gdf_surf[\"elevation\"].iloc[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Since `xhgis.watershed_delineation` extracts the nearest HydroBASINS polygon, the watershed might not exactly correspond to the requested coordinates. The 023401 streamflow gauge as an associated drainage area of 708 kmÂ², which differs from our results. Streamflow will have to be adjusted using an area scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gauge_area = 708\n",
    "scaling_factor = drainage_area / gauge_area\n",
    "scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Collection of observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import hvplot.xarray\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "# For easy access to the specific streamflow data used here\n",
    "import xdatasets\n",
    "import xscen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Meteorological data\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>INFO</b>\n",
    "\n",
    "Multiple libraries could be used to perform these steps. For simplicity, this example will use the [`subset`](https://xscen.readthedocs.io/en/latest/notebooks/2_getting_started.html#Defining-the-region) and [`aggregate` modules of the xscen](https://xscen.readthedocs.io/en/latest/notebooks/2_getting_started.html#Spatial-mean) library.\n",
    "\n",
    "</div>\n",
    "\n",
    "This example will use daily ERA5-Land data hosted on the PAVICS platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction of ERA5-Land data\n",
    "meteo_ref = xr.open_dataset(\n",
    "    \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/datasets/reanalyses/day_ERA5-Land_NAM.ncml\",\n",
    "    engine=\"netcdf4\",\n",
    "    chunks={\"time\": 365, \"lon\": 50, \"lat\": 50},\n",
    ")[[\"pr\", \"tasmin\", \"tasmax\"]]\n",
    "meteo_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "This dataset covers the entire globe and more has than 70 years of data. The first step is thus to subset the dataset both spatially and temporally. For the spatial subset, the GeoDataFrame obtained earlier can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_ref = meteo_ref.sel(time=slice(\"1991\", \"2020\"))  # Temporal subsetting\n",
    "meteo_ref = xscen.spatial.subset(\n",
    "    meteo_ref, method=\"shape\", tile_buffer=2, shape=gdf\n",
    ")  # Spatial subsetting, with a buffer of 2 grid cells\n",
    "meteo_ref = meteo_ref.assign_coords({\"crs\": meteo_ref.crs})\n",
    "meteo_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(1, 1, 1)\n",
    "meteo_ref.tasmin.isel(time=0).plot(ax=ax)\n",
    "gdf.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Since GR4JCN is a lumped model, we need to average the meteo over the watershed. This will be accomplished using `xscen.spatial_mean`. Multiple methods exist, but for shapefiles, the most accurate is \"xesmf\", which calls upon [xESMF's SpatialAverager](https://pangeo-xesmf.readthedocs.io/en/latest/notebooks/Spatial_Averaging.html) to accurately follow each grid cell's representation over the polygon. It can be very slow for detailed polygons with lots of vertices, but the \"simplify_tolerance\" argument helps speed up that process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "meteo_ref = xscen.spatial_mean(\n",
    "    meteo_ref,\n",
    "    method=\"xesmf\",\n",
    "    region={\"method\": \"shape\", \"shape\": gdf},\n",
    "    simplify_tolerance=0.1,\n",
    ")\n",
    "\n",
    "# Some housekeeping to associate the HYBAS_ID dimension (coming from the shapefile) to other coordinates added from the GeoDataFrame.\n",
    "for c in meteo_ref.coords:\n",
    "    if len(meteo_ref[c].dims) == 0:\n",
    "        if c not in [\"lon\", \"lat\"]:\n",
    "            meteo_ref = meteo_ref.drop_vars([c])\n",
    "        else:\n",
    "            meteo_ref[c] = meteo_ref[c].expand_dims(\"HYBAS_ID\")\n",
    "meteo_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Raven expects temperatures in Celsius and precipitation in millimetres, but they currently are in CF-compliant Kelvin and kg m-2 s-1, respectively. Once again, `xscen` can be used for that operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_ref = xscen.utils.change_units(\n",
    "    meteo_ref, {\"tasmax\": \"degC\", \"tasmin\": \"degC\", \"pr\": \"mm\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save (necessary for model calibration)\n",
    "meteo_ref.to_netcdf(notebook_folder / \"_data\" / \"meteo.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Hydrometric data\n",
    "\n",
    "Gauge streamflow data from the Quebec government can be accessed through the `xdatasets` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "qobs = (\n",
    "    xdatasets.Query(\n",
    "        **{\n",
    "            \"datasets\": {\n",
    "                \"deh\": {\n",
    "                    \"id\": [\"023401\"],\n",
    "                    \"variables\": [\"streamflow\"],\n",
    "                }\n",
    "            },\n",
    "            \"time\": {\"start\": \"1991-01-01\", \"end\": \"2020-12-31\"},\n",
    "        }\n",
    "    )\n",
    "    .data.squeeze()\n",
    "    .load()\n",
    ").sel(spatial_agg=\"watershed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Once again, some housekeeping is required on the metadata to make sure that xHydro understands the dataset.\n",
    "qobs[\"id\"].attrs[\"cf_role\"] = \"timeseries_id\"\n",
    "qobs = qobs.rename({\"id\": \"station_id\"})\n",
    "qobs[\"streamflow\"].attrs = {\n",
    "    \"long_name\": \"Streamflow\",\n",
    "    \"units\": \"m3 s-1\",\n",
    "    \"standard_name\": \"water_volume_transport_in_river_channel\",\n",
    "    \"cell_methods\": \"time: mean\",\n",
    "}\n",
    "for c in qobs.coords:\n",
    "    if len(qobs[c].dims) == 0 and \"time\" not in c:\n",
    "        qobs[c] = qobs[c].expand_dims(\"station_id\")\n",
    "\n",
    "qobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qobs.streamflow.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "As specified earlier, streamflow observations need to be modified to account for the differences in watershed sizes between the gauge and the polygon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(keep_attrs=True):\n",
    "    qobs[\"streamflow\"] = qobs[\"streamflow\"] * scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save (necessary for model calibration)\n",
    "qobs.to_netcdf(notebook_folder / \"_data\" / \"qobs.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Preparation and calibration of the hydrological model (xhydro.modelling)\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>INFO</b>\n",
    "\n",
    "For more information on this section and available options, consult the [Hydrological modelling module page](https://xhydro.readthedocs.io/en/latest/notebooks/hydrological_modelling.html).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xhydro.modelling.calibration import perform_calibration\n",
    "from xhydro.modelling.obj_funcs import get_objective_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "The `perform_calibration` requires a `model_config` argument that allows it to build the corresponding hydrological model. All the required information has been acquired in previous sections, so it is only a matter of filling in the entries of the RavenPy model.\n",
    "\n",
    "For simplification matters, the meteorological station's elevation will be set as the real watershed's average elevation. Computing grid cell elevation in ERA5-Land is not always trivial and is not within the scope of this example. Similarly, snow water equivalent is not currently available on PAVICS' database, so \"AVG_ANNUAL_SNOW\" was roughly estimated using [Brown & Brasnett (2010)](https://ccin.ca/ccw/snow/overview/references)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_config = {\n",
    "    \"model_name\": \"GR4JCN\",\n",
    "    \"parameters\": [0.529, -3.396, 407.29, 1.072, 16.9, 0.947],\n",
    "    \"drainage_area\": drainage_area,\n",
    "    \"elevation\": elevation,\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": \"1991-01-01\",\n",
    "    \"end_date\": \"2020-12-31\",\n",
    "    \"qobs_path\": notebook_folder / \"_data\" / \"qobs.nc\",\n",
    "    \"alt_names_flow\": \"streamflow\",\n",
    "    \"meteo_file\": notebook_folder / \"_data\" / \"meteo.nc\",\n",
    "    \"data_type\": [\"TEMP_MAX\", \"TEMP_MIN\", \"PRECIP\"],\n",
    "    \"alt_names_meteo\": {\"TEMP_MIN\": \"tasmin\", \"TEMP_MAX\": \"tasmax\", \"PRECIP\": \"pr\"},\n",
    "    \"meteo_station_properties\": {\n",
    "        \"ALL\": {\"elevation\": elevation, \"latitude\": latitude, \"longitude\": longitude}\n",
    "    },\n",
    "    \"rain_snow_fraction\": \"RAINSNOW_DINGMAN\",\n",
    "    \"evaporation\": \"PET_HARGREAVES_1985\",\n",
    "    \"global_parameter\": {\"AVG_ANNUAL_SNOW\": 100.00},\n",
    "}\n",
    "model_config[\"qobs\"] = xr.open_dataset(model_config[\"qobs_path\"]).streamflow.values\n",
    "\n",
    "# Parameter bounds for GR4JCN\n",
    "bounds_low = [0.01, -15.0, 10.0, 0.0, 1.0, 0.0]\n",
    "bounds_high = [2.5, 10.0, 700.0, 7.0, 30.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calibration / validation period\n",
    "mask_calib = xr.where(qobs.time.dt.year <= 2010, 1, 0).values\n",
    "mask_valid = xr.where(qobs.time.dt.year > 2010, 1, 0).values\n",
    "\n",
    "# Model calibration\n",
    "best_parameters, best_simulation, best_objfun = perform_calibration(\n",
    "    model_config,\n",
    "    \"kge\",\n",
    "    bounds_low=bounds_low,\n",
    "    bounds_high=bounds_high,\n",
    "    evaluations=150,\n",
    "    algorithm=\"DDS\",\n",
    "    mask=mask_calib,\n",
    "    sampler_kwargs=dict(trials=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The real KGE should be computed from a validation period, using `get_objective_function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_objective_function(\n",
    "    qobs=model_config[\"qobs\"],\n",
    "    qsim=best_simulation,\n",
    "    obj_func=\"kge\",\n",
    "    mask=mask_valid,\n",
    ").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(15, 10))\n",
    "xr.open_dataset(model_config[\"qobs_path\"]).streamflow.hvplot(\n",
    "    color=\"k\", line_width=3\n",
    ") * best_simulation.streamflow.hvplot(color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "## Calculation of hydroclimatological indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xclim\n",
    "\n",
    "import xhydro as xh\n",
    "import xhydro.frequency_analysis as xhfa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Non-frequential indicators\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>INFO</b>\n",
    "\n",
    "For more information on this section and available options, consult the [Climate change analysis page](https://xhydro.readthedocs.io/en/latest/notebooks/climate_change.html).\n",
    "\n",
    "Custom indicators in `xHydro` are built by following the YAML formatting required by `xclim`. More information is available [in the xclim documentation](https://xclim.readthedocs.io/en/latest/api.html#yaml-file-structure). The list of Yaml IDs is available [here](https://xclim.readthedocs.io/en/stable/indicators.html).\n",
    "\n",
    "</div>\n",
    "\n",
    "For a climate change impact analysis, the typical process to compute non-frequential indicators would be to:\n",
    "\n",
    "1. Define the indicators either through the `xclim` functionalities shown below or through a [YAML file](https://xclim.readthedocs.io/en/latest/api.html#yaml-file-structure).\n",
    "2. Call `xhydro.indicators.compute_indicators`, which would produce annual results through a dictionary, where each key represents the requested frequencies.\n",
    "3. Call `xhydro.cc.climatological_op` on each entry of the dictionary to compute the 30-year average.\n",
    "4. Recombine the datasets.\n",
    "\n",
    "However, if the annual results are not required, `xhydro.cc.produce_horizon` can bypass steps 2 to 4 and alleviate a lot of hassle. It accomplishes that by removing the `time` axis and replacing it for a `horizon` dimension that represents a slice of time. In the case of seasonal or monthly indicators, a corresponding `season` or `month` dimension is also added.\n",
    "\n",
    "We will compute the mean summer flow (an annual indicator) and mean monthly flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = [\n",
    "    # Mean summer flow\n",
    "    xclim.core.indicator.Indicator.from_dict(\n",
    "        data={\n",
    "            \"base\": \"stats\",\n",
    "            \"input\": {\"da\": \"streamflow\"},\n",
    "            \"parameters\": {\"op\": \"mean\", \"indexer\": {\"month\": [6, 7, 8]}},\n",
    "        },\n",
    "        identifier=\"qmoy_summer\",\n",
    "        module=\"hydro\",\n",
    "    ),\n",
    "    # Mean monthly flow\n",
    "    xclim.core.indicator.Indicator.from_dict(\n",
    "        data={\n",
    "            \"base\": \"stats\",\n",
    "            \"input\": {\"da\": \"streamflow\"},\n",
    "            \"parameters\": {\"op\": \"mean\", \"freq\": \"MS\"},\n",
    "        },\n",
    "        identifier=\"qmoy_monthly\",\n",
    "        module=\"hydro\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "ds_indicators = xh.cc.produce_horizon(\n",
    "    best_simulation, indicators=indicators, periods=[\"1991\", \"2020\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "### Frequency analysis\n",
    "\n",
    "<div class=\"alert alert-info\"> <b>INFO</b>\n",
    "\n",
    "For more information on this section and available options, consult the [Local frequency analysis page](https://xhydro.readthedocs.io/en/latest/notebooks/local_frequency_analysis.html).\n",
    "\n",
    "</div>\n",
    "\n",
    "A frequency analysis typically follows these steps:\n",
    "\n",
    "1. Get the raw data needed for the analysis, such as annual maximums, through `xhydro.indicators.get_yearly_op`.\n",
    "2. Call `xhfa.local.fit` to obtain the parameters for a specified number of distributions, such as Gumbel, GEV, and Pearson-III.\n",
    "3. (Optional) Call `xhfa.local.criteria` to obtain goodness-of-fit parameters.\n",
    "4. Call `xhfa.local.parametric_quantiles` to obtain the return levels.\n",
    "\n",
    "We will compute the 20 and 100-year annual maximums, as well as the 2-year minimum 7-day summer flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "qref_max = xh.indicators.get_yearly_op(\n",
    "    best_simulation,\n",
    "    op=\"max\",\n",
    "    timeargs={\"annual\": {}},\n",
    "    missing=\"pct\",\n",
    "    missing_options={\"tolerance\": 0.15},\n",
    ")\n",
    "qref_min = xh.indicators.get_yearly_op(\n",
    "    best_simulation,\n",
    "    op=\"min\",\n",
    "    window=7,\n",
    "    timeargs={\"summer\": {\"date_bounds\": [\"05-01\", \"11-30\"]}},\n",
    "    missing=\"pct\",\n",
    "    missing_options={\"tolerance\": 0.15},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(15, 10))\n",
    "qref_max.streamflow_max_annual.dropna(\n",
    "    \"time\", how=\"all\"\n",
    ").hvplot() * qref_min.streamflow7_min_summer.dropna(\"time\", how=\"all\").hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency analysis applied on multiple distributions\n",
    "params_max = xhfa.local.fit(\n",
    "    qref_max,\n",
    "    distributions=[\"genextreme\", \"gumbel_r\", \"norm\", \"pearson3\"],\n",
    "    method=\"MLE\",\n",
    "    min_years=20,\n",
    ").compute()\n",
    "params_min = xhfa.local.fit(\n",
    "    qref_min,\n",
    "    distributions=[\"genextreme\", \"gumbel_r\", \"norm\", \"pearson3\"],\n",
    "    method=\"MLE\",\n",
    "    min_years=20,\n",
    ").compute()\n",
    "\n",
    "params_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "While not foolproof, the best fit can be identified using the Bayesian Information Criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_max = xhfa.local.criteria(qref_max, params_max)\n",
    "criteria_max = str(\n",
    "    criteria_max.isel(\n",
    "        scipy_dist=criteria_max.streamflow_max_annual.sel(criterion=\"bic\")\n",
    "        .argmin()\n",
    "        .values\n",
    "    ).scipy_dist.values\n",
    ")  # Get the best fit as a string\n",
    "print(f\"Best distribution for the annual maxima: {criteria_max}\")\n",
    "\n",
    "criteria_min = xhfa.local.criteria(qref_min, params_min)\n",
    "criteria_min = str(\n",
    "    criteria_min.isel(\n",
    "        scipy_dist=criteria_min.streamflow7_min_summer.sel(criterion=\"bic\")\n",
    "        .argmin()\n",
    "        .values\n",
    "    ).scipy_dist.values\n",
    ")  # Get the best fit as a string\n",
    "print(f\"Best distribution for the summer 7-day minima: {criteria_min}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "data_max = xhfa.local._prepare_plots(\n",
    "    params_max, xmin=1, xmax=1000, npoints=50, log=True\n",
    ")\n",
    "data_max[\"streamflow_max_annual\"].attrs[\"long_name\"] = \"streamflow\"\n",
    "pp = xhfa.local._get_plotting_positions(qref_max[[\"streamflow_max_annual\"]])\n",
    "\n",
    "data_max.streamflow_max_annual.sel(scipy_dist=criteria_max).hvplot(\n",
    "    x=\"return_period\", grid=True, groupby=[], logx=True, color=\"k\"\n",
    ") * pp.hvplot.scatter(\n",
    "    x=\"streamflow_max_annual_pp\",\n",
    "    y=\"streamflow_max_annual\",\n",
    "    grid=True,\n",
    "    groupby=[],\n",
    "    logx=True,\n",
    "    color=\"k\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "data_min = xhfa.local._prepare_plots(\n",
    "    params_min, xmin=1, xmax=1000, npoints=50, log=True\n",
    ")\n",
    "data_min[\"streamflow7_min_summer\"].attrs[\"long_name\"] = \"streamflow\"\n",
    "pp = xhfa.local._get_plotting_positions(qref_min[[\"streamflow7_min_summer\"]])\n",
    "\n",
    "data_min.streamflow7_min_summer.sel(scipy_dist=criteria_min).hvplot(\n",
    "    x=\"return_period\", grid=True, groupby=[], logx=True, color=\"k\"\n",
    ") * pp.hvplot.scatter(\n",
    "    x=\"streamflow7_min_summer_pp\",\n",
    "    y=\"streamflow7_min_summer\",\n",
    "    grid=True,\n",
    "    groupby=[],\n",
    "    logx=True,\n",
    "    color=\"k\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computation of return levels\n",
    "rl_max = xhfa.local.parametric_quantiles(\n",
    "    params_max.sel(scipy_dist=criteria_max).expand_dims(\"scipy_dist\"), t=[20, 100]\n",
    ").squeeze()\n",
    "rl_min = xhfa.local.parametric_quantiles(\n",
    "    params_min.sel(scipy_dist=criteria_min).expand_dims(\"scipy_dist\"), t=[2], mode=\"min\"\n",
    ").squeeze()\n",
    "rl_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"20-year annual maximum: {np.round(rl_max.streamflow_max_annual.sel(return_period=20).values, 1)} mÂ³/s\"\n",
    ")\n",
    "print(\n",
    "    f\"100-year annual maximum: {np.round(rl_max.streamflow_max_annual.sel(return_period=100).values, 1)} mÂ³/s\"\n",
    ")\n",
    "print(\n",
    "    f\"2-year minimum 7-day summer flow: {np.round(rl_min.streamflow7_min_summer.values, 1)} mÂ³/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "## Future streamflow simulations\n",
    "\n",
    "### Future meteorological data\n",
    "\n",
    "Now that we have access to a calibrated hydrological model and historical indicators, we can perform the climate change analysis. This example will use a set of CMIP6 models that have been bias adjusted using ERA5-Land, for consistency with the reference product. Specifically, we will use the ESPO-G6-R2 dataset, also hosted on PAVICS. While it is recommended to use multiple emission scenarios, this example will only use the SSP2-4.5 simulations from 14 climate models.\n",
    "\n",
    "We can mostly reuse the same code as above. One difference is that climate models often use custom calendars that need to be converted back to standard ones. Once again, `xscen` can be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_CAS_FGOALS-g3_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_CMCC_CMCC-ESM2_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_CSIRO-ARCCSS_ACCESS-CM2_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_CSIRO_ACCESS-ESM1-5_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_DKRZ_MPI-ESM1-2-HR_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_INM_INM-CM5-0_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_MIROC_MIROC6_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_MPI-M_MPI-ESM1-2-LR_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_MRI_MRI-ESM2-0_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_NCC_NorESM2-LM_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_CNRM-CERFACS_CNRM-ESM2-1_ssp245_r1i1p1f2_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_NIMS-KMA_KACE-1-0-G_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_NOAA-GFDL_GFDL-ESM4_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "    \"day_ESPO-G6-E5L_v1.0.0_CMIP6_ScenarioMIP_NAM_BCC_BCC-CSM2-MR_ssp245_r1i1p1f1_1950-2100.ncml\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_climate(model):\n",
    "    meteo_sim = xr.open_dataset(\n",
    "        f\"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/datasets/simulations/bias_adjusted/cmip6/ouranos/ESPO-G/ESPO-G6-E5Lv1.0.0/{model}\",\n",
    "        engine=\"netcdf4\",\n",
    "        chunks={\"time\": 365, \"lon\": 50, \"lat\": 50},\n",
    "    )\n",
    "    meteo_sim = xscen.spatial.subset(\n",
    "        meteo_sim, method=\"shape\", tile_buffer=2, shape=gdf\n",
    "    )\n",
    "    meteo_sim = meteo_sim.assign_coords({\"crs\": meteo_sim.crs})\n",
    "    meteo_sim = xscen.spatial_mean(\n",
    "        meteo_sim,\n",
    "        method=\"xesmf\",\n",
    "        region={\"method\": \"shape\", \"shape\": gdf},\n",
    "        simplify_tolerance=0.1,\n",
    "    )\n",
    "    for c in meteo_sim.coords:\n",
    "        if len(meteo_sim[c].dims) == 0:\n",
    "            if c not in [\"lon\", \"lat\"]:\n",
    "                meteo_sim = meteo_sim.drop_vars([c])\n",
    "            else:\n",
    "                meteo_sim[c] = meteo_sim[c].expand_dims(\"HYBAS_ID\")\n",
    "\n",
    "    meteo_sim = xscen.utils.change_units(\n",
    "        meteo_sim, {\"tasmax\": \"degC\", \"tasmin\": \"degC\", \"pr\": \"mm\"}\n",
    "    )\n",
    "\n",
    "    # Manage calendars\n",
    "    meteo_sim = xscen.utils.clean_up(\n",
    "        meteo_sim,\n",
    "        convert_calendar_kwargs={\"calendar\": \"standard\", \"align_on\": \"random\"},\n",
    "        missing_by_var={\"tasmin\": \"interpolate\", \"tasmax\": \"interpolate\", \"pr\": 0},\n",
    "    )\n",
    "\n",
    "    return meteo_sim\n",
    "\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    out = _prepare_climate(model)\n",
    "    out.to_netcdf(notebook_folder / \"_data\" / f\"meteo_sim{i}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.pr.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Future streamflow data\n",
    "\n",
    "Once again, the same code as before can be roughly reused here, but with `xhydro.modelling.hydrological_model`. The main difference is that the best parameters can be used when setting up the hydrological model, and that the dates are the full range going to 2100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import xhydro.modelling as xhm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"model_name\": \"GR4JCN\",\n",
    "    \"parameters\": np.array(best_parameters),\n",
    "    \"drainage_area\": drainage_area,\n",
    "    \"elevation\": elevation,\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": \"1991-01-01\",\n",
    "    \"end_date\": \"2099-12-31\",\n",
    "    \"qobs_path\": notebook_folder / \"_data\" / \"qobs.nc\",\n",
    "    \"alt_names_flow\": \"streamflow\",\n",
    "    \"meteo_file\": \"meteo_sim.nc\",\n",
    "    \"data_type\": [\"TEMP_MAX\", \"TEMP_MIN\", \"PRECIP\"],\n",
    "    \"alt_names_meteo\": {\"TEMP_MIN\": \"tasmin\", \"TEMP_MAX\": \"tasmax\", \"PRECIP\": \"pr\"},\n",
    "    \"meteo_station_properties\": {\n",
    "        \"ALL\": {\"elevation\": elevation, \"latitude\": latitude, \"longitude\": longitude}\n",
    "    },\n",
    "    \"rain_snow_fraction\": \"RAINSNOW_DINGMAN\",\n",
    "    \"evaporation\": \"PET_HARGREAVES_1985\",\n",
    "    \"global_parameter\": {\"AVG_ANNUAL_SNOW\": 100.00},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model execution\n",
    "for i in range(len(models)):\n",
    "    model_cfg = deepcopy(model_config)\n",
    "    model_cfg[\"meteo_file\"] = notebook_folder / \"_data\" / f\"meteo_sim{i}.nc\"\n",
    "    qsim = xhm.hydrological_model(model_cfg).run()\n",
    "    qsim.to_netcdf(notebook_folder / \"_data\" / f\"qsim{i}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "qsim.streamflow.hvplot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
