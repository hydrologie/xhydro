{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "641ee068-d430-49c0-8ea2-c9ef63070ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "This code retrieves data from the Canadian Regional Climate Model V5 driven by CMIP6 global climate models (CRCM5-CMIP6).\n",
    "https://www.ouranos.ca/en/ouranos-climate-data/crcm5-cmip6\n",
    "\n",
    "To optimize storage efficiency, this code generates a Zarr file for each pilot, time step, and data type (historical or projections).\n",
    "\n",
    "It should be noted that some variables are instantaneous, meaning the value corresponds to the stated hour. However, there are others at half-hour intervals, which represent the average, minimum, or maximum value for the time period (for example, an average flux over 3 hours). To optimize storage, all variables at half-hour intervals are shifted to the previous instantaneous time step, e.g., a value at 4:30 is set at 3:00. This is not a problem for the purpose of this dataset, as this variable will be aggregated at a daily timestep. However, it might be more appropriate to shift it to the next instantaneous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "230037a7-7a52-40b0-b2e0-60a1fc41996b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import dask\n",
    "import numpy as np\n",
    "import requests\n",
    "import xarray as xr\n",
    "import zarr\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "3af30f30-c035-42d2-b0f7-caf14ca6f8b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_urls_from_catalog(catalog_url, list_url=None):\n",
    "    \"\"\"\n",
    "    Create a list of .nc file URLs from the given catalog URL.\n",
    "\n",
    "    This function fetches the content of the catalog URL, parses it to find all links,\n",
    "    and recursively processes sub-catalogs to gather .nc file URLs.\n",
    "\n",
    "    Args:\n",
    "        catalog_url (str): The URL of the THREDDS catalog.\n",
    "        list_url (list, optional): A list to store the collected .nc file URLs. Defaults to None.\n",
    "    Returns:\n",
    "        list: A list of URLs pointing to .nc files in the catalog.\n",
    "    \"\"\"\n",
    "\n",
    "    if list_url is None:\n",
    "        list_url = []\n",
    "\n",
    "    response = requests.get(catalog_url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the links in the catalog\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "\n",
    "    for link in links:\n",
    "        href = link[\"href\"]\n",
    "        if any(sbt in href for sbt in [\"twitcher\", \"https:\", \"fx\"]) or not href:\n",
    "            continue\n",
    "        # Recursively download from sub-catalogs\n",
    "        if href.endswith(\"catalog.html\"):\n",
    "            new_catalog_url = os.path.join(os.path.dirname(catalog_url), href)\n",
    "            get_urls_from_catalog(new_catalog_url, list_url)\n",
    "        elif href.endswith(\".nc\"):\n",
    "            url = f\"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/dodsC/{href.split('=')[1]}\"\n",
    "            list_url.append(url)\n",
    "    return list_url\n",
    "\n",
    "\n",
    "def create_dict_from_urls(list_url):\n",
    "    \"\"\"\n",
    "    Create a nested dictionary from a list of .nc file URLs.\n",
    "\n",
    "    This function processes each URL in the list, extracts relevant parts from the filename,\n",
    "    and organizes them into a nested dictionary structure.\n",
    "\n",
    "    Args:\n",
    "        list_url (list): A list of .nc file URLs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A nested dictionary where the keys are extracted parts of the filenames.\n",
    "    \"\"\"\n",
    "    nc_dict = {}\n",
    "    for nc_file in list_url:\n",
    "        parts = nc_file.split(\"/\")[-1].split(\"_\")\n",
    "\n",
    "        model = parts[2]  # pilots\n",
    "        ts = parts[8]  # ts\n",
    "        ssp = parts[3]  # ssp\n",
    "        member = parts[4]  # member\n",
    "        version = parts[7]  # version\n",
    "        var = parts[0]  # variable\n",
    "\n",
    "        if model not in nc_dict:\n",
    "            nc_dict[model] = {}\n",
    "        if ts not in nc_dict[model]:\n",
    "            nc_dict[model][ts] = {}\n",
    "        if ssp not in nc_dict[model][ts]:\n",
    "            nc_dict[model][ts][ssp] = {}\n",
    "        if member not in nc_dict[model][ts][ssp]:\n",
    "            nc_dict[model][ts][ssp][member] = {}\n",
    "        if version not in nc_dict[model][ts][ssp][member]:\n",
    "            nc_dict[model][ts][ssp][member][version] = {}\n",
    "        if var not in nc_dict[model][ts][ssp][member][version]:\n",
    "            nc_dict[model][ts][ssp][member][version][var] = []\n",
    "\n",
    "        nc_dict[model][ts][ssp][member][version][var].append(nc_file)\n",
    "    return nc_dict\n",
    "\n",
    "\n",
    "def calculate_time_steps(file_url, ts):\n",
    "    \"\"\"\n",
    "    Calculate the number of time steps from the file name.\n",
    "\n",
    "    Args:\n",
    "        file_url (str): The URL of the .nc file containing the date range.\n",
    "        ts (str): The time step interval ('day', '1hr', '3hr').\n",
    "\n",
    "    \"\"\"\n",
    "    if ts == \"day\":\n",
    "        ts_ = 24\n",
    "    elif ts == \"1hr\":\n",
    "        ts_ = 1\n",
    "    elif ts == \"3hr\":\n",
    "        ts_ = 3\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid time step interval: {ts}\")\n",
    "\n",
    "    match = re.search(r\"(\\d{12})-(\\d{12})\\.nc\", file_url)\n",
    "    if match:\n",
    "        start_date_str, end_date_str = match.groups()\n",
    "        start_date = datetime.strptime(start_date_str, \"%Y%m%d%H%M\")\n",
    "        end_date = datetime.strptime(end_date_str, \"%Y%m%d%H%M\")\n",
    "\n",
    "        # Calculate the number of time steps hour intervals\n",
    "        time_steps = int((end_date - start_date) / timedelta(hours=ts_)) + 1\n",
    "        return time_steps\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_file(url, store, offset, ssp, member):\n",
    "    \"\"\"\n",
    "    Process a NetCDF file and store the data in a Zarr group.\n",
    "\n",
    "    This function opens a NetCDF file from the given URL, filters the data based on the specified\n",
    "    longitude and latitude bounds, and stores the filtered data in a Zarr group.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the NetCDF file to be processed.\n",
    "        store (str): The Zarr store where the data will be stored.\n",
    "        offset (int): The offset to be applied to the time slices.\n",
    "        ssp (str): The SSP scenario identifier.\n",
    "        member (str): The member identifier.\n",
    "        lon_min (float): The minimum longitude for filtering.\n",
    "        lon_max (float): The maximum longitude for filtering.\n",
    "        lat_min (float): The minimum latitude for filtering.\n",
    "        lat_max (float): The maximum latitude for filtering.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(url)\n",
    "\n",
    "    ds = ds.where(\n",
    "        (ds.lon >= lon_min)\n",
    "        & (ds.lon <= lon_max)\n",
    "        & (ds.lat >= lat_min)\n",
    "        & (ds.lat <= lat_max),\n",
    "        drop=True,\n",
    "    ).compute()\n",
    "\n",
    "    variable = list(ds.data_vars.keys())[0]\n",
    "    da = ds[variable]\n",
    "\n",
    "    group = zarr.group(store)\n",
    "\n",
    "    index_ssp = np.where(group[\"ssp\"][:] == ssp)[0][0]\n",
    "    index_member = np.where(group[\"member\"][:] == member)[0][0]\n",
    "\n",
    "    slices = dask.array.core.slices_from_chunks(dask.array.empty_like(da).chunks)\n",
    "    for slice_ in slices:\n",
    "        time_slice, *rest = slice_\n",
    "        time_slice = slice(time_slice.start + offset, time_slice.stop + offset)\n",
    "        target_slice = (time_slice,) + tuple(rest)\n",
    "        group[variable][target_slice + (index_ssp, index_member)] = da[slice_].values\n",
    "\n",
    "\n",
    "def process_expanding(variable, url, store, offset):\n",
    "    \"\"\"\n",
    "    Process and store data from a NetCDF file in a Zarr group with an expanding offset.\n",
    "\n",
    "    This function opens a NetCDF file from the given URL, extracts the specified variable,\n",
    "    and stores the data in a Zarr group with an offset applied to the time dimension.\n",
    "\n",
    "    Args:\n",
    "        variable (str): The name of the variable to be processed.\n",
    "        url (str): The URL of the NetCDF file to be processed.\n",
    "        store (str): The Zarr store where the data will be stored.\n",
    "        offset (int): The offset to be applied to the time dimension.\n",
    "    \"\"\"\n",
    "    group = zarr.group(store)\n",
    "    ds = xr.open_dataset(url)\n",
    "    da = ds[variable]\n",
    "\n",
    "    slice_ = (slice(offset, da.shape[0] + offset),)\n",
    "\n",
    "    values = da.values\n",
    "\n",
    "    nh = (values[1] - values[0]) / np.timedelta64(1, \"h\")\n",
    "\n",
    "    # This is necessary to align the variables in the time dimension. Check the source.\n",
    "    if nh == 3:  # TODO Implement for different time steps\n",
    "        minu = [dt.astype(object).minute for dt in values.astype(\"datetime64[m]\")]\n",
    "        if 30 in minu:\n",
    "            values = values - np.timedelta64(90, \"m\")\n",
    "        group[variable][slice_] = values\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"This function currently does not support time steps different from 3 hours\"\n",
    "        )\n",
    "\n",
    "\n",
    "def decode_encode_attr(v):\n",
    "    if isinstance(v, bytes):\n",
    "        v = v.decode(\"utf-8\")\n",
    "    return xr.backends.zarr.encode_zarr_attr_value(v)\n",
    "\n",
    "\n",
    "def run_parallel(pair, store, ssp, member):\n",
    "\n",
    "    process_file(url=pair[0], store=store, offset=pair[1], ssp=ssp, member=member)\n",
    "    # process_expanding(variable='time', url=pair[0], store=store, offset= pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0a0cbfe-f17a-4376-8391-36937e77ada9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Fetching and processing catalog URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c78d8ea-3826-486a-be9f-11ec661114b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this section, the code fetches the content of the catalog URL, parses it to find all links, and recursively processes sub-catalogs to gather `.nc` file URLs. This process may take several minutes, so it is recommended to save the resulting dictionary for easier manipulation and future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4699b47-2f6a-4d63-8f99-de6d8c2b6d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_url = \"https://pavics.ouranos.ca/twitcher/ows/proxy/thredds/catalog/birdhouse/disk2/ouranos/CORDEX/CMIP6/DD/NAM-12/OURANOS/catalog.html\"\n",
    "list_url = get_urls_from_catalog(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e96cf2c-da96-4c57-ae97-c61b89284347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dict_url = create_dict_from_urls(list_url)\n",
    "# Save dictionary to a JSON file\n",
    "with open(\"./dic_nc.json\", \"w\") as file:\n",
    "    json.dump(dict_url, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a914d3d8-f56a-45f9-88cd-5e08af0b334f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with open(\"./dic_nc.json\") as file:\n",
    "    dict_url = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6baa9163-81dc-46d7-b024-d9e79c896282",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#2. Create an empty zarr\n",
    "\n",
    "An empty Zarr file with the desired structure must be created first to download the data in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61cf79e0-35d7-4c9b-a002-5856e74f9786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In this section, specify the pilot, time step, type (historical or projections), and variables to be downloaded. Additionally, define the desired spatial extent. This section also requires identifying the final length of the Zarr file (**len_time**). An example of how this can be obtained is provided, but it is recommended to double-check this variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79d64a5-ad5d-45f3-aa1c-1f1b10d23d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "directory = \"./CRCM5-CMIP6/\"\n",
    "\n",
    "pilot = \"CNRM-ESM2-1\"\n",
    "time_step = \"3hr\"\n",
    "typ = \"ssp\"  #'ssp' or 'historical'\n",
    "\n",
    "VARIABLES = [\n",
    "    \"prra\",\n",
    "    \"prsn\",\n",
    "    \"snw\",\n",
    "    \"clwvi\",\n",
    "    \"prw\",\n",
    "]\n",
    "\n",
    "# Desired spatial extent\n",
    "# This is primarily done to facilitate data manipulation; it may not improve download time\n",
    "lat_min, lat_max = 43, 50\n",
    "lon_min, lon_max = -82, -71\n",
    "\n",
    "# Double-check\n",
    "if typ == \"historical\":\n",
    "    dic4len = dict_url[pilot][time_step][\"historical\"][\"r1i1p1f2\"][\"v1-r1\"][\"clwvi\"]\n",
    "else:\n",
    "    dic4len = dict_url[pilot][time_step][\"ssp126\"][\"r1i1p1f2\"][\"v1-r1\"][\"clwvi\"]\n",
    "len_time = np.sum(\n",
    "    np.array([calculate_time_steps(n, time_step) for n in dic4len])\n",
    ")  # heads-up: this line might not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb623297-0a64-4a35-9457-0a580fc32a59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If the above variables are modified, the dictionary `ATTRS_ARRAY_DIMENSIONS` must be updated to assign the coordinates on which the new variables depend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56477a8f-536e-46e3-bb83-cc1a7ebf8cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "COORDINATES = [\n",
    "    \"time\",\n",
    "    \"rlat\",\n",
    "    \"rlon\",\n",
    "    \"lat\",\n",
    "    \"lon\",\n",
    "]\n",
    "NEW_COOR = [\"ssp\", \"member\"]\n",
    "\n",
    "ATTRS_ARRAY_DIMENSIONS = {\n",
    "    \"prra\": [\"time\", \"rlat\", \"rlon\", \"ssp\", \"member\"],\n",
    "    \"prsn\": [\"time\", \"rlat\", \"rlon\", \"ssp\", \"member\"],\n",
    "    \"snw\": [\"time\", \"rlat\", \"rlon\", \"ssp\", \"member\"],\n",
    "    \"clwvi\": [\"time\", \"rlat\", \"rlon\", \"ssp\", \"member\"],\n",
    "    \"prw\": [\"time\", \"rlat\", \"rlon\", \"ssp\", \"member\"],\n",
    "    \"time\": [\"time\"],\n",
    "    \"rlat\": [\"rlat\"],\n",
    "    \"rlon\": [\"rlon\"],\n",
    "    \"ssp\": [\"ssp\"],\n",
    "    \"member\": [\"member\"],\n",
    "    \"lat\": (\"rlat\", \"rlon\"),\n",
    "    \"lon\": (\"rlat\", \"rlon\"),\n",
    "}\n",
    "\n",
    "EXPANDING = {\"time\"}\n",
    "\n",
    "missing_value = np.nan\n",
    "\n",
    "ATTRS_DROP = {\n",
    "    \"driving_experiment_id\",\n",
    "    \"driving_variant_label\",\n",
    "    \"variable_id\",\n",
    "    \"history\",\n",
    "    \"tracking_id\",\n",
    "    \"version_realization\",\n",
    "    \"_ChunkSizes\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f568a7f0-00e8-473c-b4c3-42416844a365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create the zarr group where the data will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6f1170-cde4-4bb3-9bb3-2e8208062797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "store = os.path.join(\n",
    "    directory, \"test_fixed_\" + pilot + \"_\" + time_step + \"_\" + typ + \".zarr\"\n",
    ")\n",
    "group = zarr.group(store, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6168b6c7-1ac8-4035-a3a6-8c2289a55182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2.1. Static Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e69745a-7314-48dd-94f6-11d508c1f044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Static metadata, such as 'frequency', 'variable_id', 'units', etc., is assigned to the Zarr file using some .nc files as templates. It may be necessary to modify the `dic_vars_templates` variable according to the pilot or the desired time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1353cfd3-b4e3-47c8-8a83-ca6a68449875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dic_vars_templates = dict_url[pilot][time_step][\"historical\"][\"r1i1p1f2\"][\"v1-r1\"]\n",
    "templates_list = [\n",
    "    dic_vars_templates[li][0] for li in dic_vars_templates if li in VARIABLES\n",
    "]  # It takes the first .nc file for each variable\n",
    "\n",
    "ds_templ = xr.open_dataset(templates_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70b48247-8fac-468a-b825-38aa7968fd9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, the global metadata, along with the metadata for variables and coordinates, is added to the Zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82589341-5651-420b-b9cd-914e8f8d5454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "global_attrs = {\n",
    "    k: decode_encode_attr(v) for k, v in ds_templ.attrs.items() if k not in ATTRS_DROP\n",
    "}\n",
    "attrs = {\".zattrs\": global_attrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a27e538-e225-4963-b8fe-1da4000503b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in templates_list:\n",
    "    temp_i = xr.open_dataset(file)\n",
    "    variable = list(temp_i.data_vars.keys())[0]\n",
    "    dataset = temp_i[variable]\n",
    "    attrs[variable] = {\n",
    "        k: decode_encode_attr(v)\n",
    "        for k, v in dataset.attrs.items()\n",
    "        if k not in ATTRS_DROP\n",
    "    }\n",
    "    attrs[variable][\"_ARRAY_DIMENSIONS\"] = ATTRS_ARRAY_DIMENSIONS[variable]\n",
    "    attrs[variable][\"missing_value\"] = missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7dafe8-4974-4eda-a581-e1babc902f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for coordinate in COORDINATES:\n",
    "    dataset = ds_templ[coordinate]\n",
    "    attrs[coordinate] = {\n",
    "        k: decode_encode_attr(v)\n",
    "        for k, v in dataset.attrs.items()\n",
    "        if k not in ATTRS_DROP\n",
    "    }\n",
    "    attrs[coordinate][\"_ARRAY_DIMENSIONS\"] = ATTRS_ARRAY_DIMENSIONS.get(coordinate, [])\n",
    "    if \"_FILL_VALUE\" in attrs[coordinate]:\n",
    "        attrs[coordinate][\"missing_value\"] = missing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3c6d7e-41da-4b49-8634-d86ac3e73861",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for new_coor in NEW_COOR:\n",
    "    attrs[new_coor] = {}\n",
    "    attrs[new_coor][\"_ARRAY_DIMENSIONS\"] = ATTRS_ARRAY_DIMENSIONS.get(new_coor, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c5454df-a6f0-4fb8-b140-4f7f999d6463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.2. Pre-allocate Zarr groups for variables and coordinates\n",
    "The size of the Zarr file is defined based on the templates from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd35bc2-9da7-474b-84dd-e9d7b14a3d25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if typ == \"ssp\":\n",
    "    ssp_list = np.array(\n",
    "        [key for key in dict_url[pilot][time_step].keys() if key != \"historical\"]\n",
    "    )\n",
    "elif typ == \"historical\":\n",
    "    ssp_list = np.array([\"historical\"])\n",
    "else:\n",
    "    raise ValueError(f\"Invalid type: {typ}. It must be ssp or historical\")\n",
    "\n",
    "member_list = np.array(\n",
    "    list(dict_url[pilot][time_step][ssp_list[0]].keys())\n",
    ")  # TODO: check that all the ssp has the same members. For now, we use the members of the first ssp\n",
    "\n",
    "n_ssp = len(ssp_list)\n",
    "n_member = len(\n",
    "    dict_url[pilot][time_step][ssp_list[0]].keys()\n",
    ")  # TODO: check that all the ssp has the same members"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9181dbf-6f8e-47a3-b0d9-94a503e126c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Crop `ds_templ` dataset based on the specified longitude and latitude boundaries and defined the size of the Zarr file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b6588f8-816b-4346-8e60-fb0e26def520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "templ = ds_templ.where(\n",
    "    (ds_templ.lon >= lon_min)\n",
    "    & (ds_templ.lon <= lon_max)\n",
    "    & (ds_templ.lat >= lat_min)\n",
    "    & (ds_templ.lat <= lat_max),\n",
    "    drop=True,\n",
    ").compute()\n",
    "full_shape = (len_time, templ.rlat.shape[0], templ.rlon.shape[0], n_ssp, n_member)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8c4d725-54ad-467e-9aec-d52df9ad91ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The size of each variable and the coordinates are allocated according to the template of each variable, as well as the size of the number of SSPs and members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d2f682-4cc4-4f94-b209-760c5a47184d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in templates_list:\n",
    "    temp_i = xr.open_dataset(file)\n",
    "    ds3 = temp_i.where(\n",
    "        (temp_i.lon >= lon_min)\n",
    "        & (temp_i.lon <= lon_max)\n",
    "        & (temp_i.lat >= lat_min)\n",
    "        & (temp_i.lat <= lat_max),\n",
    "        drop=True,\n",
    "    )  # .compute()\n",
    "    variable = list(ds3.data_vars.keys())[0]\n",
    "    dataset = ds3[variable]\n",
    "\n",
    "    chunks = (templ.time.shape[0], templ.rlon.shape[0], templ.rlat.shape[0], 1, 1)  ###\n",
    "\n",
    "    v = group.empty(\n",
    "        dataset.name,\n",
    "        shape=full_shape,\n",
    "        dtype=dataset.dtype,\n",
    "        chunks=chunks,\n",
    "        overwrite=True,\n",
    "    )\n",
    "    v.attrs.update(attrs[dataset.name.lstrip(\"/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a0c8351-91de-4da8-96a5-8cc5f15b3c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for coord in COORDINATES:  # TODO: this is slowish. Maybe parallelize on 1 machine.\n",
    "\n",
    "    dataset = templ[coord]\n",
    "    print(\"handle\", coord)\n",
    "\n",
    "    if coord not in EXPANDING:\n",
    "        shape = dataset.shape\n",
    "    else:\n",
    "        shape = (full_shape[0],) + dataset.shape[1:]\n",
    "    shape = tuple(int(x) for x in shape)\n",
    "\n",
    "    v = group.empty(\n",
    "        dataset.name, shape=shape, dtype=dataset.dtype, overwrite=True, chunks=shape\n",
    "    )\n",
    "    if coord not in EXPANDING:\n",
    "        # Replace some static data\n",
    "        v[:] = np.array(dataset)\n",
    "        if coord in [\"lat\", \"lon\"]:\n",
    "            v[:, :] = np.array(dataset)\n",
    "    v.attrs.update(attrs[dataset.name.lstrip(\"/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da0ed3ac-0038-4426-a716-211dfd18d47b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "v = group.empty(\n",
    "    \"ssp\", shape=(n_ssp,), dtype=np.dtype(\"U10\"), overwrite=True, chunks=(1,)\n",
    ")\n",
    "v[:] = ssp_list\n",
    "v.attrs.update(attrs[\"ssp\"])\n",
    "\n",
    "v = group.empty(\n",
    "    \"member\", shape=(n_member,), dtype=np.dtype(\"U10\"), overwrite=True, chunks=(1,)\n",
    ")\n",
    "v[:] = member_list\n",
    "v.attrs.update(attrs[\"member\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40482f86-96cf-4db4-8f8b-74708529e4cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Download data\n",
    "Pay close attention to the `TODO` lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3bad0a1-e9c8-4c95-990c-4362cfb360aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Parallelize_download_crcm5\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "for ssp in ssp_list:\n",
    "    for member in member_list:  # TODO: some members get error when downloading\n",
    "        for key in VARIABLES:\n",
    "            try:\n",
    "                names = dict_url[pilot][time_step][ssp][member][\"v1-r1\"][\n",
    "                    key\n",
    "                ]  # TODO create another loop for \"v1-r1\" when there is more than one version\n",
    "\n",
    "                full_shape = np.array(\n",
    "                    [calculate_time_steps(n, time_step) for n in names]\n",
    "                )\n",
    "\n",
    "                csum = np.cumsum(np.array(full_shape))\n",
    "                offsets = np.concatenate([np.array([0]), csum[:-1]])\n",
    "\n",
    "                rdd1 = sc.parallelize(names)\n",
    "                rdd2 = sc.parallelize(offsets)\n",
    "\n",
    "                zipped_rdd = rdd1.zip(rdd2)\n",
    "                # run_parallel((names[0],offsets[0]), store, ssp, member) #This line help to debug the code\n",
    "\n",
    "                zipped_rdd.foreach(lambda pair: run_parallel(pair, store, ssp, member))\n",
    "                print(\"ssp:\" + ssp + \", member:\" + member + \", var:\" + key + \", done\")\n",
    "            except:\n",
    "                print(\n",
    "                    \"############### ERROR: ssp:\"\n",
    "                    + ssp\n",
    "                    + \", member:\"\n",
    "                    + member\n",
    "                    + \", var:\"\n",
    "                    + key\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97b76cb0-e5f6-4766-811f-a801083d8289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Add dates to the zarr file based on `.nc` files in the variable `names` from the last loop of the previous code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "012fe946-b40f-4998-ae45-a04e1a710ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(len(names)):\n",
    "    process_expanding(variable=\"time\", url=names[i], store=store, offset=offsets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4933499-2875-480e-8f25-079a1091b801",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "zarr.consolidate_metadata(store)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01.1_download_CRCM5-CMIP6",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
